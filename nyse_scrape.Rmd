---
title: "NYSE Scraping and Analysis"
subtitle: "C2 - Term Project"
author: "Son N. Nguyen"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  rmdformats::downcute:
    self_contained: true
    lightbox: true
    gallery: false
    highlight: tango
    code_folding: hide
---

```{r, echo=F}
knitr::opts_chunk$set(fig.align = "center", warning = FALSE, message = FALSE, fig.width=12, fig.height=8) 
```

## Introduction

*All the displays and data in my report represent the state of NYSE after closing on 14 Dec 2021.*

My topic for this research was scraping and analyzing the equities listed on the New York Stock Exchange. 
I utilized [Eoddata](http://eoddata.com/symbols.aspx) as my source, and I scraped HTML for my work using the GadgetSelector tool and the methods used in class, because the page didn't have an accessible API from which I could have extracted the JSON object of each table.

**Note: The full script used for this project can be found under my [Github repo](https://github.com/nszoni/c2-nyse-term-project), which includes everything needed to reproduce the workflow here.**

>My advice is to clone the repository if you want the run the script.

```{r setup, include=F}

#setwd("~/Desktop/repos/c2-nyse-term-project")

# Loading packages with pacman
if (!require("pacman")) {
  install.packages("pacman")
}

pacman::p_load(tidyverse, rvest, data.table,
               xml2, stringr, logger, rmdformats)

rooturl <- 'http://eoddata.com/stocklist/NYSE/'

```

## Scraping the EODDATA

### Get One Page

To begin, I needed to develop a function to get one page of the ABC, which I could then use with 'lapply' to apply to all the other pages. In a nutshell, the function below does the following: **(1) parses the provided website URL, (2) iterates through all the columns of the current page and puts them in an initialized list, (3) creates a data frame from the list** (+1) all with debugging logging.

```{r get data}

get_one_page <- function(url){
  
  t <- read_html(url)
  
  logger::log_info('URL parsed')
  
  #init empty list
  mydata <- list()
  
  tryCatch(
    {
      for (i in 1:9){
      mydata[[i]] <- t %>% 
        html_nodes(paste0('#ctl00_cph1_divSymbols td:nth-child(',i,')')) %>% html_text() %>% list()
      }
      logger::log_info("Got all the columns!")
    },
    error = function(e){
        stop("Couldn't get all the columns.")
    }
  )
  #loop through columns of the table and append
  
  df <- data.frame(mydata)
  
  logger::log_info('Dataframe formed')
  
  return(df)
}

```

### Get Ticker Table

To retrieve every listed symbol on the page, I needed to apply the previously stated function to all letters of the English ABC. To do this, I created another function that generates URLs for all of the pages, applies the one-page-getter function, and then binds all of the pages together as a single data frame.

**This took about 50 seconds to get them all.**

```{r urls, message=F, warning=F, results='hide', eval = FALSE}

get_all_stock <- function(rooturl){
  
  start.time <- Sys.time()
  
  #init empty list of urls
  urls <- list()
  
  #generate urls for pages
  for (i in 1:length(letters)){
    urls[[i]] <- paste0(rooturl,letters[i],".htm")
  }
  
  logger::log_info('URLs generated for page {letters[i]}!')
  
  #get all and bind it together
  stocklist <- rbindlist(lapply(urls, get_one_page))
  
  logger::log_info('Got all the data!')
  
  end.time <- Sys.time()
  time.taken <- end.time - start.time
  print(time.taken)
  
  return(stocklist)
}

stocklist_raw <- get_all_stock(rooturl)

```

```{r, include=F, eval=F}

#rename columns
colnames(stocklist_raw) <- c('code', 'name', 'high', 'low', 'close', 'volume', 'change_absolute', 'direction', 'change_percent')

```

```{r unzip, echo=F}

#Here I downloaded some html pages in advance and compressed it to the repo
#unzip if uncompressed folder doesn't exists

if (!file.exists("htmls/")){
  unzip("htmls_compressed.zip", exdir="./htmls/")
}

```

### Get Fundamentals tables

My next goal was to obtain all of the "Fundamentals" tables from each stock subpage, thus I needed to **generate the subpage URL for each ticker and obtain the entire table as a list, then incrementally append it as a row to a data frame**. Knowing this, I did exactly what I needed to do and pasted together a root URL and all of the ticker numbers, which was sufficient to obtain the pages.

When I first started looking through those pages and extracting the information I needed, I ran across a problem that forced me to rethink my strategy. The issue was that if I tried to request each of the almost 4,000 subpages and tables, I received a timeout response. As a result, **I downloaded all of the subpages and iterated over those local files, which were more efficient to have on hand**. If the page for a ticker has already been downloaded locally, the written for loop skips the download; otherwise, it obtains it from the parsed URL.

Another potential difficulty was that the stocks listed on the **NYSE** are updated very often (new IPOs are announced on a daily basis), therefore I needed a script to add the missing fundamental tables based on the scraped list of tickers. 

*Please note that while new stock is added incrementally, so we don't have to update every record when executing the script; however, previously downloaded stock pages become stale, and we must download them again to update them. One disadvantage of the gradual upgrade is that it saves time rather than having to download all of the pages from scratch, which takes hours.*

```{r, message=F, warning=F, eval = FALSE, echo=F}

rooturl2 <- 'https://eoddata.com/stockquote/NYSE/'

colnames <- c("sector", "industry", "pe_ratio", "peg_ratio", "eps", "divyield", "ptb", "pts", "ebitda", "shares", "market_cap", "52wk_range")

tickers <- stocklist_raw[["code"]]

#download all subpages for tickers incrementally

for (i in 1:length(tickers)){
  if (!file.exists(paste0("htmls/",tickers[i],".htm"))){
      url <- paste0(rooturl2,tickers[i],".htm")
      download.file(url, destfile = paste0('htmls/',tickers[i],'.htm'), quiet = T)
      logger::log_info("Stock {tickers[i]} HTML downloaded")
  }else{
    logger::log_info("Page are already downloaded, skipping.")
  }
}

```

Moving on to extracting the essentials, I iterated over the list of.htm files in my working folder, flattening out each stock's table as a row, then incrementally binding each row together. **With both the main symbols table for each ticker and all the fundamental tables in one data frame, I was finally able to merge them, resulting in my raw NYSE dataset**.

```{r, eval=F}

get_all_fundamentals <- function(tickers){
  
  start.time <- Sys.time()
  
  #init empty dataframe
  fundamentals <- NULL
  
  #get fundamentals for list of tickers
  for (i in 1:length(tickers)){
    subpage <- paste0("htmls/",tickers[i],".htm")
    f <- read_html(subpage)
    values <- f %>% html_nodes('#ctl00_cph1_divFundamentals td:nth-child(2)') %>% html_text()
    fundamentals <- rbind(fundamentals, data.frame(matrix(unlist(values), ncol=length(values), byrow=F)))
    logger::log_info("Got table for ticker {tickers[i]}!")
  }
  
  colnames(fundamentals) <- colnames
  
  end.time <- Sys.time()
  time.taken <- end.time - start.time
  print(time.taken)
  
  return(fundamentals)
  
}

fundamentals <- get_all_fundamentals(tickers)

nyse_raw <- cbind(stocklist_raw, fundamentals)

```

```{r merge, echo=F}

#write to csv
#write.csv(nyse_raw, 'nyse_raw.csv', row.names=FALSE)

#read csv
nyse_raw <- read.csv("nyse_raw.csv", sep = ',')

```

### Data Cleaning

Arriving to data cleaning, I did the following changes:

1. Standardize number formatting for numbers like 1.1B, 1.1M, 1.1K to display million values.
2. Separate the `X52_week_range` column displaying an interval to two separate columns with the lower and upper bound.
3. Change formatting of relative changes in prices so that it reflects the direction (multiply with -1 if the absolute change below zero)
4. Cast and apply rounding for each column
5. Replace empty strings with NAs
6. **Drop warrants and all other option-like abstractions** which were not comparable to standard stocks

```{r formatting}

#formatter to million
formatter <- function(col){
  num <- gsub('B', 'e3', col)
  num <- gsub('M', '', num)
  num <- gsub('K', 'e-3', num)
  format(as.numeric(num), scientific = FALSE, big.mark = ",")
}

#fix number formatting
cols <- c("ebitda", "shares", "market_cap")
nyse_raw[cols] <- lapply(nyse_raw[cols], formatter)

#cleanup thousand separators, and cast
nyse_clean <- nyse_raw %>%
          mutate_all(funs(gsub(",", "", .)), select(., high:X52wk_range)) %>%
          separate(X52wk_range, c('52wk_range_low', '52wk_range_high'), sep = " - ") %>% 
          mutate(high = as.numeric(high),
                 low = as.numeric(low),
                 close = as.numeric(close),
                 change_absolute = round(as.numeric(change_absolute), 2),
                 volume = as.integer(volume)/10^6,
                 change_percent = round((ifelse(change_absolute < 0, (-1)*as.numeric(change_percent), as.numeric(change_percent))/100), 2),
                 direction = ifelse(change_percent < 0, "down", ifelse(change_percent > 0, "up", "same")),
                 pe_ratio = round(as.numeric(pe_ratio), 2),
                 peg_ratio = round(as.numeric(peg_ratio), 2),
                 eps = round(as.numeric(eps), 2),
                 divyield = round(as.numeric(divyield)/100, 2),
                 ptb = round(as.numeric(ptb), 2),
                 pts = round(as.numeric(pts), 2),
                 ebitda = round(as.numeric(ebitda), 2),
                 shares = round(as.numeric(shares), 2),
                 market_cap = round(as.numeric(market_cap), 2),
                 `52wk_range_low` = as.numeric(`52wk_range_low`),
                 `52wk_range_high` = as.numeric(`52wk_range_high`),
                 )
         
nyse_clean[nyse_clean == ""] <- NA

#drop warrants and all other abstractions which are not standard stocks
nyse_clean <- nyse_clean[!grepl("\\[", nyse_clean$name),]
nyse_clean <- nyse_clean[!grepl(".W", nyse_clean$code),]

```

## NYSE Analysis

Since the data in nature of itself was cross-sectional not time series, it restricted my options during the visualization. 

### Market Capitalization 

First, I looked at the NYSE sectors with the biggest market capitalization. 

**With a total market valuation of almost $800 billion, the Energy sector won the top spot, followed by Technology and Finance**. This could be explained by the assumption that our civilization's desire for renewable energy and oil is increasing. Technology plays a significant role in the digitization of the pandemic, but finance is still required. 

<br>

```{r dataviz}

#sectors with the largest market cap
nyse_clean %>% select(c('sector', 'market_cap')) %>%  
  na.omit() %>% 
  arrange(desc(market_cap)) %>%
  slice(1:5) %>%
  ggplot() +
    geom_col(aes(reorder(sector, market_cap), market_cap, fill = sector)) +
  theme_bw() +
  coord_flip() +
  labs(title = "Sectors with the largest market cap (M)",
       y = "Market Cap",
       x = "",
       caption = "Source: eoddata.com") +
  scale_fill_viridis_d(name = "Sector")

```
<br>

### Industry Gains

Second, I was wondering as to which industries are now outperforming others in terms of gains. Overall, the **Telecommunications industry** increased by over 15%, which was more than double the EDP services, which came in second place, closely followed by Electronic Distribution**. Again, these displays show how COVID has changed our economy and perspectives.

<br>

```{r}

#Best Performing Industries
nyse_clean %>% filter(!is.na(industry)) %>% 
  arrange(desc(change_percent)) %>%
  slice(1:5) %>%
  ggplot() +
    geom_bar(aes(reorder(industry, change_percent), change_percent), fill = "slateblue2", color = "slateblue4", stat = "summary", fun = "mean") +
  theme_bw() +
  coord_flip() +
  labs(title = "Best Performing Industries",
       y = "Gain",
       x = "",
       caption = "Source: eoddata.com") +
  scale_y_continuous(labels=scales::percent)

```
<br>

### Sector Advancers and Decliners

Third, I wanted to observe how each sector's advancers and decliners were distributed. The figure below shows that on the day of my analysis, all stocks in the **Utilities and Metals sector** were in decline. The former is a bit counter-intuitive in that it is a non-cyclical industry, and when the economy is doing well, a downturn is usually seen (which is highly unlikely in the current times). 

In contrast, we can see a **smaller share of advancers** in the Finance and Industrial Goods industries. Other patterns are appearing, such as a drop in the transportation industry, where the virus has imposed border restrictions once again, and lower demand for travel in general. To summarize, market sentiment is negative, and all industries have been impacted by the present economic downturn.


```{r}

#Sector Sentiments
nyse_clean %>% filter(!is.na(sector)) %>% 
ggplot(aes(fill=direction, x=sector)) + 
  geom_bar(position='fill') +
  labs(title = 'Sector Sentiments',
       caption = 'Source: eoddata.com',
       x = '',
       y = 'Share') +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_viridis_d(name = 'Sentiment') +
  theme_bw() +
  coord_flip()

```
<br>

### Most Active Stocks

In terms of individual stock comparisons, I put together a chart that shows the stocks with the highest activity on the current day. **We can detect Ford (F) and AMC Entertainment (AMC) among the leaders, which is unsurprising given that these are low-cost, Reddit-fueled "meme-stocks." **Also, AT&T (T) and Pfizer (PFE)** appear there, both of which are fueled by the pandemic's present need for the services of these companies.

```{r}

#stocks with the most volume
nyse_clean %>% 
  arrange(desc(volume)) %>%
  slice(1:5) %>%
  ggplot() +
    geom_col(aes(reorder(code, volume), volume), fill = "slateblue2", color = "slateblue4") +
  theme_bw() +
  coord_flip() +
  labs(title = "Stocks with the most volume (M)",
       y = "Volume",
       x = "",
       caption = "Source: eoddata.com")

```
<br>

### Growh Stocks

Stocks with a high dividend yield are referred to as "Growth Stocks" by financial professionals. Businesses that have reached the apex of the marginal product curve and are unable to expand significantly. Price stability is preferred by those wishing to invest in these equities above higher potential returns. I intended to highlight which NYSE stocks provide the greatest (relative) dividends to their shareholders. **RiverNorth Specialty Finance Corporation CF (RSF) is at the top of the list, followed by Trane Technologies PLC (TT) and Entravision Communications Corp (EVC)**.

```{r}

#stocks with the largest dividend yields
nyse_clean %>% 
  arrange(desc(divyield)) %>%
  slice(1:5) %>%
  ggplot() +
    geom_col(aes(reorder(code, divyield), divyield), fill = "slateblue2", color = "slateblue4") +
  theme_bw() +
  coord_flip() +
  labs(title = "Stocks with the largest divident yields",
       y = "Yield",
       x = "",
       caption = "Source: eoddata.com") +
  scale_y_continuous(labels=scales::percent)

```
<br>

### Top Gainers and Losers

Finally, let's take a look at today's top gainers and losers. **Terminix Global Holdings Inc (TMX), which provides residential and commercial services, and BigBear.ai Holdings Inc (BBAI), which specializes in mergers and acquisitions, were the best-performing stocks on 14 December, with price increases greater than 15%**. 


```{r}

#Top Gainers
nyse_clean %>% 
  arrange(desc(change_percent)) %>%
  slice(1:5) %>%
  ggplot() +
    geom_col(aes(reorder(code, change_percent), change_percent), fill = "springgreen3", color = "springgreen4") +
  theme_bw() +
  coord_flip() +
  labs(title = "Top Gainers",
       y = "Gain",
       x = "",
       caption = "Source: eoddata.com") +
  scale_y_continuous(labels=scales::percent)

```
<br>

Moving on to the losers of the day, **Navios Maritime Holdings Inc. (NM), a company responsible for overseas logistics, closed at the bell with a loss of more than 25%, followed by Planet Labs (a result of a SPAC initiative from dMY Technologies) and Oscar Health (OSCR), a private insurance company**. We can infer patterns that are sensitively determined by the restrictions and hardship imposed by the pandemic's upward trending infection rates.

```{r}

#Top Losers
nyse_clean %>% 
  arrange(change_percent) %>%
  slice(1:5) %>%
  ggplot() +
    geom_col(aes(reorder(code, -change_percent), change_percent), fill = "firebrick2", color = "firebrick4") +
  theme_bw() +
  coord_flip() +
  labs(title = "Top Losers",
       y = "Loss",
       x = "",
       caption = "Source: eoddata.com") +
  scale_y_continuous(labels=scales::percent)

```

# Conclusion

To sum up, we can deduce information about the current economy from all of the visuals I gave. The rapidly spreading Omnicron variant and the rapidly declining numbers of the epidemic appear to have placed pessimism on the NYSE as well. Companies involved in the creation of vaccinations are thriving, whereas transportation and logistical services are lagging. In terms of sectors and industries, they continue to adapt to the expanding demands of digitization, with Technology and Telecommunications essentially leading the market. Aside from that, "meme-stocks" continue to be in great demand in terms of activity.
