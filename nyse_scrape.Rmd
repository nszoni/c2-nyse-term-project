---
title: "C2 - Term Project - NYSE Scraping and Analysis"
author: "Son . Nguyen"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  prettydoc::html_pretty:
  toc: true
  theme: architect
  highlight: github
---

```{r, echo=F}
knitr::opts_chunk$set(fig.align = "center", warning = FALSE, message = FALSE) 
```

## Introduction

*All the display, and data in my report represent the state of NYSE after closing on 14 Dec 2021.*

For this project, my subject was scraping and analyzing the stocks included in the NYSE.
My used source was [Eoddata](http://eoddata.com/symbols.aspx), and I used the GadgetSelector tool along with the studied methods to scrape html for my work as the page didn't have an accessible API from which I could have extracted the JSON object of each table.

**Note: The full script used for this project can be found under my [Github repo](https://github.com/nszoni/c2-nyse-term-project), which includes everything which makes the workflow reproducible.**

```{r setup, include=F}

setwd("~/Desktop/repos/c2-nyse-term-project")

# Loading packages with pacman
if (!require("pacman")) {
  install.packages("pacman")
}

pacman::p_load(tidyverse, rvest, data.table,
               xml2, stringr, logger, heatmaply)

rooturl <- 'http://eoddata.com/stocklist/NYSE/'

```

## Get One Page

Firstly, somehow I needed to write a function to get one page of the ABC, and which I can apply on all the other pages with `lapply`. The function below does the following in short: (1) Parses the given website URL, (2) iterates through all the columns of the current page and stores it in an initialized list, (3) constructs a dataframe from the list (+1) does it all with logging for debugging purposes.

```{r get data}

get_one_page <- function(url){
  
  t <- read_html(url)
  
  logger::log_info('URL parsed')
  
  #init empty list
  mydata <- list()
  
  tryCatch(
    {
      for (i in 1:9){
      mydata[[i]] <- t %>% 
        html_nodes(paste0('#ctl00_cph1_divSymbols td:nth-child(',i,')')) %>% html_text() %>% list()
      }
      logger::log_info("Got all the columns!")
    },
    error = function(e){
        stop("Couldn't get all the columns.")
    }
  )
  #loop through columns of the table and append
  
  df <- data.frame(mydata)
  
  logger::log_info('Dataframe formed')
  
  return(df)
}

```

# Get Ticker Table

Next, I needed to apply the function written in previously to all letters of the english ABC to get every listed symbols on the website. For that, I wrote a function which generates URLs for all the pages, and applies the one-page-getter function, then binds all the pages together as a one unified dataframe. 

**This took about 50 second to get them all**

```{r urls, message=F, warning=F, results='hide', eval = FALSE}

get_all_stock <- function(rooturl){
  
  start.time <- Sys.time()
  
  #init empty list of urls
  urls <- list()
  
  #generate urls for pages
  for (i in 1:length(letters)){
    urls[[i]] <- paste0(rooturl,letters[i],".htm")
  }
  
  logger::log_info('URLs generated for page {letters[i]}!')
  
  #get all and bind it together
  stocklist <- rbindlist(lapply(urls, get_one_page))
  
  logger::log_info('Got all the data!')
  
  end.time <- Sys.time()
  time.taken <- end.time - start.time
  print(time.taken)
  
  return(stocklist)
}

stocklist_raw <- get_all_stock(rooturl)

```

```{r, include=F, eval=F}

#rename columns
colnames(stocklist_raw) <- c('code', 'name', 'high', 'low', 'close', 'volume', 'change_absolute', 'direction', 'change_percent')

```

```{r unzip, echo=F}

#Here I downloaded some html pages in advance and compressed it to the repo
#unzip if uncompressed folder doesn't exists

if (!file.exists("htmls/")){
  unzip("htmls_compressed.zip", exdir="./htmls/")
}

```

# Get Fundamentals tables

My next objective was to get all the "Fundamentals" tables in each stock subpage, therefore, I needed to somehow generate the subpage of each ticker and get the whole table as a list, append it as a row to a dataframe incremntally. Understanding that, I did exactly what I needed to do and pasted together a rooturl and all the ticker codes which happened to be enough to get the pages. 

Getting into those pages and extracting the necessary information, I stumbled upon an issue which quickly changed my approach. The problem was that if I tried to request every subpage and table of nearly 4,000 tickers, I got a response of timeout. Thus, I downloaded each subpage and iterated through those local files which happened to be more efficient having them locally. The written for loop checks if the page for a ticker was already downloaded locally, if so, skips the download, otherwise just gets it from the parsed URL.

```{r, message=F, warning=F, eval = FALSE, echo=F}

rooturl2 <- 'https://eoddata.com/stockquote/NYSE/'

colnames <- c("sector", "industry", "pe_ratio", "peg_ratio", "eps", "divyield", "ptb", "pts", "ebitda", "shares", "market_cap", "52wk_range")

tickers <- stocklist_raw[["code"]]

#download all subpages for tickers incrementally

for (i in 1:length(tickers)){
  if (!file.exists(paste0("htmls/",tickers[i],".htm"))){
      url <- paste0(rooturl2,tickers[i],".htm")
      download.file(url, destfile = paste0('htmls/',tickers[i],'.htm'), quiet = T)
      logger::log_info("Stock {tickers[i]} HTML downloaded")
  }else{
    logger::log_info("Page are already downloaded, skipping.")
  }
}

```

Moving onto extracting the fundamentals, I iterated through the list of htm files in my working folder and flatted out the table of each stock as a row, then binded together every row, incrementally. Having both the main symbols table for every ticker in one dataframe, and all the fundamental tables, finally, I could merged them together, which essentially resulted in my raw NYSE dataset.

```{r, eval=F}

get_all_fundamentals <- function(tickers){
  
  start.time <- Sys.time()
  
  #init empty dataframe
  fundamentals <- NULL
  
  #get fundamentals for list of tickers
  for (i in 1:length(tickers)){
    subpage <- paste0("htmls/",tickers[i],".htm")
    f <- read_html(subpage)
    values <- f %>% html_nodes('#ctl00_cph1_divFundamentals td:nth-child(2)') %>% html_text()
    fundamentals <- rbind(fundamentals, data.frame(matrix(unlist(values), ncol=length(values), byrow=F)))
    logger::log_info("Got table for ticker {tickers[i]}!")
  }
  
  colnames(fundamentals) <- colnames
  
  end.time <- Sys.time()
  time.taken <- end.time - start.time
  print(time.taken)
  
  return(fundamentals)
  
}

fundamentals <- get_all_fundamentals(tickers)

nyse_raw <- cbind(stocklist_raw, fundamentals)

```

```{r merge, echo=F}

#write to csv
#write.csv(nyse_raw, 'nyse_raw.csv', row.names=FALSE)

#read csv
nyse_raw <- read.csv("nyse_raw.csv", sep = ',')

```

# Data Cleaning

Arriving to data cleaning, I did the following changes:

1. Standardize number formatting for numbers like 1.1B, 1.1M, 1.1K to display million values.

```{r formatting}

#formatter to million
formatter <- function(col){
  num <- gsub('B', 'e3', col)
  num <- gsub('M', '', num)
  num <- gsub('K', 'e-3', num)
  format(as.numeric(num), scientific = FALSE, big.mark = ",")
}

#fix number formatting
cols <- c("ebitda", "shares", "market_cap")
nyse_raw[cols] <- lapply(nyse_raw[cols], formatter)

```

2. Separate the `X52_week_range` column displaying an interval to two separate columns with the lower and upper bound.
3. Change formatting of relative changes in prices so that it reflects the direction (multiply with -1 if the absolute change below zero)
4. Cast and apply rounding for each column
5. Replace empty strings with NAs

```{r casting}
#cleanup thousand separators, and cast
nyse_clean <- nyse_raw %>%
          mutate_all(funs(gsub(",", "", .)), select(., high:X52wk_range)) %>%
          separate(X52wk_range, c('52wk_range_low', '52wk_range_high'), sep = " - ") %>% 
          mutate(high = as.numeric(high),
                 low = as.numeric(low),
                 close = as.numeric(close),
                 change_absolute = round(as.numeric(change_absolute), 2),
                 volume = as.integer(volume)/10^6,
                 change_percent = round((ifelse(change_absolute < 0, (-1)*as.numeric(change_percent), as.numeric(change_percent))/100), 2),
                 direction = ifelse(change_percent < 0, "down", ifelse(change_percent > 0, "up", "same")),
                 pe_ratio = round(as.numeric(pe_ratio), 2),
                 peg_ratio = round(as.numeric(peg_ratio), 2),
                 eps = round(as.numeric(eps), 2),
                 divyield = round(as.numeric(divyield)/100, 2),
                 ptb = round(as.numeric(ptb), 2),
                 pts = round(as.numeric(pts), 2),
                 ebitda = round(as.numeric(ebitda), 2),
                 shares = round(as.numeric(shares), 2),
                 market_cap = round(as.numeric(market_cap), 2),
                 `52wk_range_low` = as.numeric(`52wk_range_low`),
                 `52wk_range_high` = as.numeric(`52wk_range_high`),
                 )
         
nyse_clean[nyse_clean == ""] <- NA

```

# NYSE Analysis

Since the data in nature of itself was cross-sectional not time series, it restricted my options during the visualization. 

First, I looked at sectors having the largest market cap in the NYSE. The Energy sector took the crown with a total market cap of over $800B, followed by Technology and Finance. This could be explained with the theory that renewable energy and oil has a growing demand in our civilization. Technology plays huge part in the digitization in the pandemic, while Finance is still needed as before.

```{r dataviz, echo=F}

#sectors with the largest market cap
nyse_clean %>% select(c('sector', 'market_cap')) %>%  
  na.omit() %>% 
  arrange(desc(market_cap)) %>%
  slice(1:5) %>%
  ggplot() +
    geom_col(aes(reorder(sector, market_cap), market_cap, fill = sector)) +
  theme_bw() +
  coord_flip() +
  labs(title = "Sectors with the largest market cap (M)",
       y = "Market Cap",
       x = "",
       caption = "Source: eoddata.com") +
  scale_fill_viridis_d(name = "Sector")

```

Second, I was also curious about which industries are in the top performers in terms of gains today. In overall, the Telecommunications industry had over 15% gain in total which was double the EDP services coming in second in the ranking, closely followeb by Electronic Distribution. Again, these displays reflect how COVID have impacted the daily activites on the market, as there is a growing demand in (1) "staying connected and online" through telecommunications, (2) delivering services (EDP), and (3) digitalization via electronics distribution.

```{r, echo=F}

#Best Performing Industries
nyse_clean %>% filter(!is.na(industry)) %>% 
  arrange(desc(change_percent)) %>%
  slice(1:5) %>%
  ggplot() +
    geom_bar(aes(reorder(industry, change_percent), change_percent), fill = "firebrick2", color = "firebrick4", stat = "summary", fun = "mean") +
  theme_bw() +
  coord_flip() +
  labs(title = "Best Performing Industries",
       y = "Gain",
       x = "",
       caption = "Source: eoddata.com") +
  scale_y_continuous(labels=scales::percent)

```
Third, I wanted to see the distribution of advancers and decliners in each sector. The below chart indicates that in the Utilities and Metal sector, all stocks were in decline on the day of my analysis with no exception. The former is a bit counterintuitive in a sense that it is a non-cyclical sector, and a downtrend usually observable when the economy performs well (which is highly unlikely in the current times). 

In contrast to that, Funeral services, and Cement-Concrete stocks were all going up. These phenomenas can be partly explained by the fact that the Utilities sector is highly cyclical, thus follows the current economy decline due to COVID19. The demand for funeral services may go up as well because of the increasing death rates as new variants of the virus starts to appear. Other patterns are also emerging, for example the decline in the transportation sector, where again, the virus has imposed restrictive measure on the border, and lower demand for travelling in general

```{r, echo=F}

#Sector Sentiments
nyse_clean %>% filter(!is.na(sector)) %>% 
ggplot(aes(fill=direction, x=sector)) + 
  geom_bar(position='fill') +
  labs(title = 'Sector Sentiments',
       caption = 'Source: eoddata.com',
       x = '',
       y = 'Share') +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_viridis_d(name = 'Sentiment') +
  theme_bw() +
  coord_flip()

```

Regarding the comparison of individual stocks, I assembled a chart which shows stocks having the most activity in the current day. Among the tops, we can recognize Ford (F), and AMC Entertainment (AMC), which is not surprising considering that these are relatively cheap and Reddit fuelled "meme-stocks". Also, AT&T (T) and Pfizer (PFE) appear there which are both driven by the current demand for these products in the pandemic.

```{r, echo=F}

#stocks with the most volume
nyse_clean %>% 
  arrange(desc(volume)) %>%
  slice(1:5) %>%
  ggplot() +
    geom_col(aes(reorder(code, volume), volume), fill = "firebrick2", color = "firebrick4") +
  theme_bw() +
  coord_flip() +
  labs(title = "Stocks with the most volume (M)",
       y = "Volume",
       x = "",
       caption = "Source: eoddata.com")

```
People in finance define stocks having a large dividend yield as "Growth Stocks". Companies which are at the peak of the marginal product curve and are not able to expand that much. Those who are looking to invest in these stock prefer price stability over larger possible gains. I wanted to show which are the stocks in NYSE which offer the largest cut from the price to their shareholders. Result indicate that  
RiverNorth Specialty Finance Corporation CF (RSF) tops the chart, followed by Trane Technologies PLC (TT) tied with Entravision Communications Corp (EVC).

```{r, echo=F}

#stocks with the largest dividends
nyse_clean %>% 
  arrange(desc(divyield)) %>%
  slice(1:5) %>%
  ggplot() +
    geom_col(aes(reorder(code, divyield), divyield), fill = "firebrick2", color = "firebrick4") +
  theme_bw() +
  coord_flip() +
  labs(title = "Stocks with the largest divident yields",
       y = "Yield",
       x = "",
       caption = "Source: eoddata.com") +
  scale_y_continuous(labels=scales::percent)

```

Lastly, we may want to look at the top gainers and losers today,


```{r, echo=F}

#Top Gainers
nyse_clean %>% 
  arrange(desc(change_percent)) %>%
  slice(1:5) %>%
  ggplot() +
    geom_col(aes(reorder(code, change_percent), change_percent), fill = "springgreen3", color = "springgreen4") +
  theme_bw() +
  coord_flip() +
  labs(title = "Top Gainers",
       y = "Gain",
       x = "",
       caption = "Source: eoddata.com") +
  scale_y_continuous(labels=scales::percent)

```

```{r, echo=F}

#Top Losers
nyse_clean %>% 
  arrange(change_percent) %>%
  slice(1:5) %>%
  ggplot() +
    geom_col(aes(reorder(code, -change_percent), change_percent), fill = "firebrick2", color = "firebrick4") +
  theme_bw() +
  coord_flip() +
  labs(title = "Top Losers",
       y = "Loss",
       x = "",
       caption = "Source: eoddata.com") +
  scale_y_continuous(labels=scales::percent)

```

# Conclusion

Behind all the visualizations, we are able to gauge information about the current economy.
