---
title: "C2 - Term Project - stocklist"
author: "Son Nam Nguyen"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  prettydoc::html_pretty:
  toc: true
  theme: architect
  highlight: github
---

## Setup

```{r pacman, echo=F, message=F}

setwd("~/Desktop/repos/c2-nyse-term-project")

# Loading packages with pacman
if (!require("pacman")) {
  install.packages("pacman")
}

pacman::p_load(tidyverse, rvest, data.table,
               xml2, stringr, logger)

rooturl <- 'http://eoddata.com/stocklist/NYSE/'

```

## Get One Page

```{r get data}

get_one_page <- function(url){
  
  t <- read_html(url)
  
  logger::log_info('URL parsed')
  
  #init empty list
  mydata <- list()
  
  tryCatch(
    {
      for (i in 1:9){
      mydata[[i]] <- t %>% 
        html_nodes(paste0('#ctl00_cph1_divSymbols td:nth-child(',i,')')) %>% html_text() %>% list()
      }
      logger::log_info("Got all the columns!")
    },
    error = function(e){
        stop("Couldn't get all the columns.")
    }
  )
  #loop through columns of the table and append
  
  df <- data.frame(mydata)
  
  logger::log_info('Dataframe formed')
  
  return(df)
}

```

# Get Ticker Table

```{r urls, message=F, warning=F}

get_all_stock <- function(rooturl){
  
  start.time <- Sys.time()
  
  #init empty list of urls
  urls <- list()
  
  #generate urls for pages
  for (i in 1:length(letters)){
    urls[[i]] <- paste0(rooturl,letters[i],".htm")
  }
  
  logger::log_info('URLs generated for page {letters[i]}!')
  
  #get all and bind it together
  stocklist <- rbindlist(lapply(urls, get_one_page))
  
  logger::log_info('Got all the data!')
  
  end.time <- Sys.time()
  time.taken <- end.time - start.time
  print(time.taken)
  
  return(stocklist)
}

stocklist_raw <- get_all_stock(rooturl)

#rename columns
colnames(stocklist_raw) <- c('code', 'name', 'high', 'low', 'close', 'volume', 'change', 'direction', 'price')

```

# Get Fundamentals tables

```{r, message=F, warning=F}

rooturl2 <- 'https://eoddata.com/stockquote/NYSE/'

colnames <- c("sector", "industry", "pe_ratio", "peg_ratio", "eps", "divyield", "ptb", "pts", "ebitda", "shares", "market_cap", "52wk_range")

tickers <- stocklist_raw[["code"]]

#download all subpages for tickers incrementally

for (i in 1:length(tickers)){
  if (!file.exists(paste0("htmls/",tickers[i],".htm"))){
      url <- paste0(rooturl2,tickers[i],".htm")
      download.file(url, destfile = paste0('htmls/',tickers[i],'.htm'), quiet = T)
      logger::log_info("Stock {tickers[i]} HTML downloaded")
  }else{
    logger::log_info("Page are already downloaded, skipping.")
  }
}

#zip-unzip

get_all_fundamentals <- function(tickers){
  
  start.time <- Sys.time()
  
  #init empty dataframe
  fundamentals <- NULL
  
  #get fundamentals for list of tickers
  for (i in 1:length(tickers)){
    subpage <- paste0("htmls/",tickers[i],".htm")
    f <- read_html(subpage)
    values <- f %>% html_nodes('#ctl00_cph1_divFundamentals td:nth-child(2)') %>% html_text()
    fundamentals <- rbind(fundamentals, data.frame(matrix(unlist(values), ncol=length(values), byrow=F)))
    logger::log_info("Got table for ticker {tickers[i]}!")
  }
  
  colnames(fundamentals) <- colnames
  
  end.time <- Sys.time()
  time.taken <- end.time - start.time
  print(time.taken)
  
  return(fundamentals)
  
}

fundamentals <- get_all_fundamentals(tickers)

```

# Merge tables

```{r merge}

nyse_raw <- cbind(stocklist_raw, fundamentals)

#write to csv
write.csv(nyse_raw, 'nyse_raw.csv')

```

# Data Cleaning

```{r transform}

#cleanup thousand separators, and cast
nyse_clean <- nyse_raw %>%
          select(-c("direction")) %>%
          mutate_all(funs(gsub(",", "", .)), select(., high:price)) %>%
          mutate(high = as.numeric(high),
                 low = as.numeric(low),
                 close = as.numeric(close),
                 change = round(as.numeric(change), 2),
                 volume = as.integer(volume),
                 price = as.numeric(price))

```

# Visualizations

```{r dataviz}

```